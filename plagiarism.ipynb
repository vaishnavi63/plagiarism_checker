{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manual path entry for every repo\n",
    "from git import Repo\n",
    "n=int(input(\"Enter the number of git repos want to clone:\"))\n",
    "for i in range (0,n):\n",
    "    a=input(\"Repo link:\")\n",
    "    b=input(\"Path:\")\n",
    "    Repo.clone_from(a,b)\n",
    "    print('Cloned',a,'to',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download path of the given repos are same as the present working directory\n",
    "import os\n",
    "from git import Repo\n",
    "cwd = os.getcwd()\n",
    "n=int(input(\"Enter the number of git repos want to clone:\"))\n",
    "for i in range (0,n):\n",
    "    a=input(\"Repo link:\")\n",
    "    c=(a.split('/'))\n",
    "    b=cwd+'\\Repo\\\\' + c[3]\n",
    "    Repo.clone_from(a,b)\n",
    "    print('Cloned',a,'to',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manual path setup \n",
    "import os\n",
    "from git import Repo\n",
    "n=int(input(\"Enter the number of git repos want to clone:\"))\n",
    "for i in range (0,n):\n",
    "    a=input(\"Repo link:\")\n",
    "    c=(a.split('/'))\n",
    "    b='Enter Path'+'\\Repo\\\\' + c[3]  #Replace Enter Path with the destination directory\n",
    "    Repo.clone_from(a,b)\n",
    "    print('Cloned',a,'to',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy\n",
    "from scipy.sparse import vstack as vstack_sparse_matrices\n",
    "from nltk.tokenize import word_tokenize \n",
    "import os\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "#function to tokenize the text file\n",
    "def get_tokenized(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "#function to stemming tokens\n",
    "def stemming(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "    tokens = get_tokenized(text) #tokenize\n",
    "    stemmer = PorterStemmer()\n",
    "    stem = stemming(tokens,stemmer) # stemming\n",
    "    return stem\n",
    "\n",
    "def training_tfs():\n",
    "    fs = []\n",
    "    i = 0\n",
    "    path = 'C:\\\\Users\\\\Vaishnavi\\\\PLAGARISM\\\\Plag\\\\files' # replace it with the repo files or target files path\n",
    "    token_dictionary = {}\n",
    "    for subdir, dirs,files in os.walk(path):\n",
    "        for file in files:\n",
    "            file_path = subdir + os.path.sep + file\n",
    "            f = open(file_path,'r')\n",
    "            lowers = f.read().lower()\n",
    "            #lowers= text.lower()\n",
    "            no_punctuation = lowers.translate(string.punctuation)\n",
    "            token_dictionary[file] = no_punctuation\n",
    "            fs.append(file)# = file\n",
    "        \n",
    "    tfidf = TfidfVectorizer(tokenizer = preprocessing(fs),stop_words = 'English')\n",
    "    tfs = tfidf.fit_transform(token_dictionary.values())\n",
    "    return tfidf,tfs,fs\n",
    "\n",
    "\n",
    "#call function plag_training to find out tfidf and tfs for the training files\n",
    "tfidf,tfs,files = plag_training.training_tfs()\n",
    "\n",
    "#open input file\n",
    "f = open(\"input.txt\")   #path of input file\n",
    "files.append(\"input.txt\")\n",
    "#convert input file data to lower case\n",
    "text = f.read().lower().translate(string.punctuation)\n",
    "\n",
    "#now get tfidf for the input file and combine with the previous model\n",
    "response= tfidf.transform([text])\n",
    "tfs_combined = vstack_sparse_matrices([tfs,response])\n",
    "\n",
    "\n",
    "#now calculate the similarity of files with each other\n",
    "mat = (tfs_combined*tfs_combined.T).A\n",
    "\n",
    "#printing out plagiarized files\n",
    "no_ofRows =  len(mat)\n",
    "\n",
    "#find out percantage plagarization for input files with training files\n",
    "for x in range(0,len(mat[0])-1):\n",
    "    if(mat[no_ofRows-1,x] > 0.8):\n",
    "        print(f\"file is plagarized with {files[x]} by {mat[no_ofRows-1][x] * 100} percantage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
